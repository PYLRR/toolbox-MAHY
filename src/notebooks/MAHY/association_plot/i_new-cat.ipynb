{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.detection.association_geodesic import squarize\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "})\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "import math\n",
    "from numpy.linalg import LinAlgError\n",
    "import pandas as pd\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "from collections import Counter\n",
    "from utils.physics.geodesic.distance import distance_point_point\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import GridSphericalSoundModel as GridSoundModel, MonthlyHomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association_geodesic import compute_candidates, update_valid_grid, update_results, load_detections, compute_grids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# paths\n",
    "CATALOG_PATH = \"/media/plerolland/akoustik/MAHY\"\n",
    "i_DETECTIONS_DIR = f\"../../../../data/detection/i_TiSSNet_raw_OBS-fixed\"\n",
    "\n",
    "# sound model definition\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "\n",
    "LAT_BOUNDS = [-13.4, -12.4]\n",
    "LON_BOUNDS = [45.25, 46.25]\n",
    "with open(\"../../../../data/detection/i_TiSSNet_raw_OBS-fixed/MAHY0/cache/grids_-13.4_-12.4_45.25_46.25_150_1_0.25_0.25.pkl\", \"rb\") as f:\n",
    "    GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTIES = pickle.load(f)\n",
    "i_GRID_TO_COORDS = np.array(GRID_TO_COORDS)\n",
    "\n",
    "\n",
    "seismic_paths = glob2.glob(\"../../../../data/MAHY/loc_3D/*.npz\")\n",
    "acoustic_to_s = {}\n",
    "for s in STATIONS:\n",
    "    depth, bathy = s.other_kwargs[\"depth\"], s.other_kwargs[\"bathy\"]\n",
    "    under_hydro = bathy - depth\n",
    "    acoustic_to_s[s] = under_hydro / 1520\n",
    "interp_seismic = {}\n",
    "for seismic_propa_path in seismic_paths:\n",
    "    depth = float(seismic_propa_path.split(\"_\")[-1].split(\"m\")[0])\n",
    "    data = np.load(seismic_propa_path)\n",
    "    seismic_propagations = data[\"values\"]\n",
    "    seismic_propagations[(seismic_propagations < 10 ** -6) | (seismic_propagations > 10 ** 6)] = np.nan\n",
    "    seismic_depths = data[\"depths\"] * 1_000\n",
    "    seismic_distances = data[\"distances\"] * 1_000\n",
    "    interp_seismic[depth] = RegularGridInterpolator(\n",
    "        (seismic_depths, seismic_distances),\n",
    "        seismic_propagations,\n",
    "        bounds_error=False,  # allow extrapolation\n",
    "        fill_value=None)\n",
    "available_depths = np.array(list(interp_seismic.keys()))\n",
    "s_to_interp = {s: interp_seismic[available_depths[np.argmin(abs(available_depths - s.other_kwargs[\"bathy\"]))]]\n",
    "               for s in STATIONS}\n",
    "\n",
    "clock_corrections = pd.read_csv(\"../../../../data/detection/TiSSNet_Pn_raw_repicked/corrections_fixed-intercept.csv\", names=[\"s\",\"intercept\",\"slope\",\"u\"], header=None).set_index('s')"
   ],
   "id": "724af32e9cfd9325",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_ASSOCIATION_SIZE = 3\n",
    "datasets = set([s.dataset for s in STATIONS])\n",
    "i_new_stations = {}\n",
    "i_idx_to_det = {}\n",
    "for dataset in datasets:\n",
    "    dets = glob2.glob(f\"{i_DETECTIONS_DIR}/{dataset}/cache/detections*.pkl\")[0]\n",
    "    with open(dets, \"rb\") as f:\n",
    "        i_DETECTIONS = pickle.load(f)\n",
    "    i_idx_det = 0\n",
    "    i_idx_to_det_local = {}\n",
    "    for idx, s in enumerate(i_DETECTIONS.keys()):\n",
    "        s.idx = idx  # indexes to store efficiently the associations\n",
    "        i_DETECTIONS[s] = list(i_DETECTIONS[s])\n",
    "        for i in range(len(i_DETECTIONS[s])):\n",
    "            i_DETECTIONS[s][i] = np.concatenate((i_DETECTIONS[s][i], [i_idx_det]))\n",
    "            i_idx_to_det_local[i_idx_det] = i_DETECTIONS[s][i]\n",
    "            i_idx_det += 1\n",
    "        i_DETECTIONS[s] = np.array(i_DETECTIONS[s])\n",
    "    i_new_stations[dataset] = list(i_DETECTIONS.keys())\n",
    "    i_idx_to_det[dataset] = i_idx_to_det_local\n",
    "\n",
    "STATIONS = i_new_stations"
   ],
   "id": "51ea9deddd3ef7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets",
   "id": "268ba55e88d7cdee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DELTA = datetime.timedelta(seconds=5)\n",
    "\n",
    "catalog = []\n",
    "for dataset in datasets:\n",
    "    association_files = glob2.glob(f\"{i_DETECTIONS_DIR}/{dataset}/cache/associations_3_*.pkl\")\n",
    "    for file in association_files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            associations = pickle.load(f)\n",
    "        for detections, valid_points in tqdm(associations):\n",
    "            valid_coords = i_GRID_TO_COORDS[valid_points.astype(np.int32)]\n",
    "            pos = np.nanmean(valid_coords, axis=0)\n",
    "            if np.any(np.isnan(pos)):\n",
    "                continue\n",
    "            catalog.append({\"latitude\":pos[0], \"longitude\":pos[1]})\n",
    "\n",
    "            dates_reception = []\n",
    "            dates = []\n",
    "            rms = []\n",
    "            for si, di in detections:\n",
    "                s = STATIONS[dataset][si]\n",
    "                if \"43\" in s.name:\n",
    "                    s.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "\n",
    "                s.other_kwargs[\"raw\"] = True\n",
    "                date = i_idx_to_det[dataset][di][0]\n",
    "                dates_reception.append(date)\n",
    "\n",
    "                dist = distance_point_point([pos[0],pos[1]], s.get_pos())\n",
    "\n",
    "                dates.append(date - datetime.timedelta(seconds=(dist / 1500)))\n",
    "\n",
    "                c = clock_corrections.loc[s.name][\"intercept\"] + clock_corrections.loc[s.name][\"slope\"] * (date-s.date_start).total_seconds() * 10**-6\n",
    "                c = datetime.timedelta(seconds=c)\n",
    "                data = s.get_manager().get_segment(date+c-DELTA,date+c+DELTA)\n",
    "                rms.append(np.log10(dist) + np.log10(np.max(data**2)))\n",
    "            date = dates[0] + np.mean(np.array(dates) - dates[0])\n",
    "            catalog[-1][\"date\"] = date\n",
    "            catalog[-1][\"dates_orig\"] = dates_reception\n",
    "            catalog[-1][\"n_stations\"] = len(detections)\n",
    "\n",
    "            SL = 10*np.log10(np.nanmean(10**np.array(rms)))\n",
    "            catalog[-1][\"SL\"] = SL"
   ],
   "id": "71092fb6e194681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "whole_df = pd.DataFrame(catalog)\n",
    "whole_df.to_pickle(\"save_with-subsets.pkl\")\n",
    "print(len(whole_df))\n",
    "\n",
    "# remove associations that are subsets of other associations\n",
    "sets_dates = whole_df[\"dates_orig\"].apply(set).tolist()\n",
    "sizes = np.array([len(s) for s in sets_dates])\n",
    "keep = np.ones(len(whole_df), dtype=bool)\n",
    "idx_4 = np.where(sizes == 4)[0]\n",
    "for i in tqdm(idx_4):\n",
    "    big_set = sets_dates[i]\n",
    "    for j in range(len(whole_df)):\n",
    "        if i == j or not keep[j]:\n",
    "            continue\n",
    "        small_set = sets_dates[j]\n",
    "        if small_set.issubset(big_set):\n",
    "            keep[j] = False\n",
    "clean_df = whole_df[keep].reset_index(drop=True)\n",
    "\n",
    "print(len(clean_df))\n",
    "\n",
    "clean_df.to_pickle(\"../../../../data/MAHY/loc_3D/i_association_catalog_clean_df.pkl\")\n",
    "\n",
    "df = clean_df.sort_values(\"date\")\n",
    "\n",
    "all_dates = [date for sublist in df[\"dates_orig\"] for date in sublist]\n",
    "date_counts = Counter(all_dates)\n",
    "duplicated_dates = {date for date, count in date_counts.items() if count > 1}\n",
    "to_drop = df[df[\"dates_orig\"].apply(lambda lst: any(date in duplicated_dates for date in lst))].index\n",
    "clean_df = df.drop(index=to_drop).reset_index(drop=True)\n",
    "\n",
    "clean_df.to_pickle(\"../../../../data/MAHY/loc_3D/i_association_catalog_clean_df_decimated.pkl\")\n",
    "print(len(clean_df))"
   ],
   "id": "8c3b307963234160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_pickle(\"../../../../data/MAHY/loc_3D/i_association_catalog_clean_df.pkl\")\n",
    "df.to_csv(\"../../../../data/MAHY/loc_3D/i_association_catalog_full.csv\", index=False, columns=[\"date\",\"latitude\",\"longitude\",\"SL\",\"n_stations\", \"dates_orig\"])\n",
    "\n",
    "\n",
    "df_N = df[(df[\"latitude\"] > -12.62) & (df[\"latitude\"] < -12.49) & (df[\"longitude\"] > 45.49) & (df[\"longitude\"] < 45.6)].sort_values(\"date\")\n",
    "df_N.to_csv(\"../../../../data/MAHY/loc_3D/i_association_catalog_North.csv\", index=False, columns=[\"date\",\"latitude\",\"longitude\",\"SL\"], float_format=\"%.4f\")\n",
    "\n",
    "df_C = df[(df[\"latitude\"] > -12.9) & (df[\"latitude\"] < -12.84) & (df[\"longitude\"] > 45.62) & (df[\"longitude\"] < 45.72)].sort_values(\"date\")\n",
    "df_C.to_csv(\"../../../../data/MAHY/loc_3D/i_association_catalog_Center.csv\", index=False, columns=[\"date\",\"latitude\",\"longitude\",\"SL\"], float_format=\"%.4f\")"
   ],
   "id": "a872bd4285af76b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.sort_values(\"mb\")\n",
    "nb = np.log10(np.arange(1,1+len(df)))[::-1]\n",
    "plt.scatter(nb,df[\"mb\"])"
   ],
   "id": "4a78f8c034be3da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7bac6cd4cce0637",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
