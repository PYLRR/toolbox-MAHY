{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook aims at applying TiSSNet on all the demo data.",
   "id": "447206cd5027c17d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.signal.make_spectrogram import make_spectrogram\n",
    "from utils.detection.TiSSNet import TiSSNet, process_batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalog_path = \"/media/plerolland/akoustik/MAHY\"\n",
    "tissnet_checkpoint = \"../../../../data/models/TiSSNet/torch_save_checked_Pn\"\n",
    "out_dir = \"../../../../data/detection/TiSSNet_Pn_raw/MAHY\"  # where files will be saved\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)  # create output directory if needed\n",
    "\n",
    "stations = StationsCatalog(catalog_path).filter_out_undated() # remove stations with no start / end dates\n",
    "\n",
    "model_det = TiSSNet()\n",
    "model_det.load_state_dict(torch.load(tissnet_checkpoint))\n",
    "\n",
    "DELTA = datetime.timedelta(seconds=3600)  # duration of segments that are given to TiSSNet\n",
    "OVERLAP = 0.02   # overlap between those segments (no link with STFT)\n",
    "STEP = (1 - OVERLAP) * DELTA\n",
    "batch_size = 1  # number of segments that are fed together to TiSSNet\n",
    "\n",
    "# parameters of peak finding (TiSSNet outputs 1 value per spectrogram time bin, we use a peak finding algorithm to save only the peaks)\n",
    "TISSNET_PROMINENCE = 0.05\n",
    "ALLOWED_ERROR_S = 5\n",
    "MIN_HEIGHT = 0.05\n",
    "\n",
    "TIME_RES = 0.5342  # duration of each spectrogram pixel in seconds\n",
    "FREQ_RES = 0.9375  # f of each spectrogram pixel in Hz\n",
    "\n",
    "device = \"cuda\"  # if there is a GPU and CUDA is installed, device can be set to \"cuda\" instead\n",
    "model_det.to(device)"
   ],
   "id": "dd5df7fb2321b1b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for station in stations:\n",
    "    station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "    manager = station.get_manager()\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\" # results are saved as stacked pickle files\n",
    "\n",
    "    print(f\"Starting detection on {manager.name}\")\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start)/STEP))\n",
    "    start_idx = 0\n",
    "    batch_dates, batch_process = [], []\n",
    "\n",
    "    # if some detection has already been run, we start where it was stopped\n",
    "    already_done = []\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    already_done.append(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        last_date = already_done[-1][0]\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    # we start detection\n",
    "    for i in tqdm(range(steps)):\n",
    "        if i < start_idx:\n",
    "            continue # this is just to fill tqdm progress bar in case we loaded an old detection file\n",
    "\n",
    "        # important : prefer index multiplication over incrementation to avoid rounding errors\n",
    "        # (i.e. seg_start = start + i * STEP is way better than seg_start = seg_start + STEP)\n",
    "        seg_start = start + i * STEP\n",
    "        seg_end = min(end, seg_start + DELTA)\n",
    "        if seg_start >= seg_end:\n",
    "            break\n",
    "\n",
    "        # add data to batch\n",
    "        data = manager.get_segment(seg_start, seg_end)\n",
    "        if len(data) / manager.sampling_f > 1:\n",
    "            spectrogram = make_spectrogram(data, manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "            if spectrogram.shape[0] > 128:\n",
    "                spectrogram = spectrogram[spectrogram.shape[0]-128:]\n",
    "            elif spectrogram.shape[0] < 128:\n",
    "                spectrogram = np.concatenate((np.zeros((128-spectrogram.shape[0], spectrogram.shape[1]), np.float32), spectrogram), axis=0)\n",
    "            spectrogram = spectrogram[np.newaxis, :, :]  # add a dummy dimension, this stands for the channel number (here we are in grayscale, i.e. only one value for each pixel)\n",
    "            input_data = Resize((128, spectrogram.shape[-1]))(torch.from_numpy(spectrogram)) # resize data\n",
    "            batch_dates.append(seg_start)\n",
    "            batch_process.append(input_data)\n",
    "\n",
    "        # check if the batch is ready to be processed\n",
    "        if len(batch_process) == batch_size or i == steps-1:\n",
    "            if batch_size > 1 and (batch_process[-1].shape != batch_process[0].shape or batch_process[-2].shape != batch_process[-1].shape):\n",
    "                # last (and probably the one before because of overlaps) batch has a last element shorter than the others, we thus make three batches\n",
    "                rlastlast = process_batch(batch_process[-2], device, model_det)\n",
    "                rlast = process_batch(batch_process[-1], device, model_det)\n",
    "                rfirst = process_batch(batch_process[:-2], device, model_det)\n",
    "                res = list(rfirst) + [rlastlast] + [rlast]\n",
    "            else:\n",
    "                res = process_batch(batch_process, device, model_det)\n",
    "\n",
    "            # now proceed to peak finding for each window to keep only the peaks\n",
    "            for i, (seg_start, r) in enumerate(zip(batch_dates, res)):\n",
    "                actual_time_res = (seg_end-seg_start).total_seconds() / res.shape[1]\n",
    "                peaks = find_peaks(r, height=0, distance=ALLOWED_ERROR_S / actual_time_res, prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * actual_time_res\n",
    "                peaks = [(seg_start + datetime.timedelta(seconds=time_s[j]), peaks[1][\"peak_heights\"][j]) for j in range(len(time_s)) if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT]\n",
    "\n",
    "                with open(out_file, \"ab\") as f:\n",
    "                    for i, (d, p) in enumerate(peaks):\n",
    "                        pickle.dump([d, p.astype(np.float16)], f)  # we write detections as a list of (date, peak probability)\n",
    "\n",
    "            batch_dates, batch_process = [], []"
   ],
   "id": "1514ed570bed70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "HEIGHT = 128\n",
    "\n",
    "def process_and_make_spec(idx, start, STEP, DELTA, end, sampling_f, HEIGHT):\n",
    "    seg_start = start + idx * STEP\n",
    "    seg_end = min(end, seg_start + DELTA)\n",
    "    data = global_manager.get_segment(seg_start, seg_end)\n",
    "    if len(data) / sampling_f <= 1:\n",
    "        return None\n",
    "\n",
    "    spec = make_spectrogram(data, global_manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "    if spec.shape[0] > HEIGHT:\n",
    "        spec = spec[-HEIGHT:]\n",
    "    elif spec.shape[0] < HEIGHT:\n",
    "        spec = np.pad(spec, ((HEIGHT - spec.shape[0], 0), (0, 0)), 'constant')\n",
    "    return (seg_start, spec[np.newaxis, :, :])\n",
    "\n",
    "_manager_cache = {}\n",
    "\n",
    "def get_manager_for_worker():\n",
    "    pid = os.getpid()\n",
    "    if pid not in _manager_cache:\n",
    "        _manager_cache[pid] = global_manager  # global_station est dÃ©fini par le process parent\n",
    "    return _manager_cache[pid]\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Starting detection on {station.name}\")\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\"\n",
    "\n",
    "    if \"43\" in station.name:\n",
    "        station.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "    station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "    manager = station.get_manager()\n",
    "    global_manager = manager  # pour les workers\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start) / STEP))\n",
    "    start_idx = 0\n",
    "\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try: last_date = pickle.load(f)[0]\n",
    "                except EOFError: break\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    with open(out_file, \"ab\") as f_out, \\\n",
    "         ProcessPoolExecutor() as executor:\n",
    "\n",
    "        for i in tqdm(range(start_idx, steps, BATCH_SIZE)):\n",
    "            idxs = [i + j for j in range(BATCH_SIZE) if (i + j) < steps]\n",
    "            results = list(executor.map(\n",
    "                process_and_make_spec,\n",
    "                idxs,\n",
    "                [start]*len(idxs),\n",
    "                [STEP]*len(idxs),\n",
    "                [DELTA]*len(idxs),\n",
    "                [end]*len(idxs),\n",
    "                [manager.sampling_f]*len(idxs),\n",
    "                [HEIGHT]*len(idxs)\n",
    "            ))\n",
    "\n",
    "            results = [r for r in results if r is not None]\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            times_loaded, spectros = zip(*results)\n",
    "\n",
    "            try:\n",
    "                batch_tensor = np.stack(spectros)\n",
    "                preds = process_batch(batch_tensor, device, model_det)\n",
    "                pairs = zip(times_loaded, preds)\n",
    "            except ValueError:\n",
    "                pairs = []\n",
    "                for time_, spec in zip(times_loaded, spectros):\n",
    "                    try:\n",
    "                        spec_tensor = np.expand_dims(spec, 0)\n",
    "                        pred = process_batch(spec_tensor, device, model_det)[0]\n",
    "                        pairs.append((time_, pred))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur dans le traitement du spectro isolÃ© : {e}\")\n",
    "\n",
    "            for seg_start, pred in pairs:\n",
    "                t_res = DELTA.total_seconds() / pred.shape[0]\n",
    "                peaks = find_peaks(pred, height=0, distance=math.ceil(ALLOWED_ERROR_S / t_res), prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * t_res\n",
    "                for j, t in enumerate(time_s):\n",
    "                    if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT:\n",
    "                        date = seg_start + datetime.timedelta(seconds=t)\n",
    "                        prob = peaks[1][\"peak_heights\"][j]\n",
    "                        pickle.dump([date, prob.astype(np.float16)], f_out)"
   ],
   "id": "e57c0397f6f465c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torchvision.transforms import Resize\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.signal.make_spectrogram import make_spectrogram\n",
    "from utils.detection.TiSSNet import TiSSNet\n",
    "from utils.detection.TiSSNet import process_batch\n",
    "\n",
    "catalog_path = \"/media/plerolland/akoustik/MAHY/MAHY.csv\"\n",
    "tissnet_checkpoint = \"../../../../data/models/i_TiSSNet/torch_save_checked-reboot-3\"\n",
    "out_dir = \"../../../../data/detection/i_TiSSNet_raw/MAHY\"  # where files will be saved\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)  # create output directory if needed\n",
    "\n",
    "stations = StationsCatalog(catalog_path).filter_out_undated() # remove stations with no start / end dates\n",
    "print(stations)\n",
    "\n",
    "model_det = TiSSNet()\n",
    "model_det.load_state_dict(torch.load(tissnet_checkpoint))\n",
    "\n",
    "DELTA = datetime.timedelta(seconds=100)  # duration of segments that are given to TiSSNet\n",
    "OVERLAP = 0.02   # overlap between those segments (no link with STFT)\n",
    "STEP = (1 - OVERLAP) * DELTA\n",
    "batch_size = 1  # number of segments that are fed together to TiSSNet\n",
    "\n",
    "# parameters of peak finding (TiSSNet outputs 1 value per spectrogram time bin, we use a peak finding algorithm to save only the peaks)\n",
    "TISSNET_PROMINENCE = 0.05\n",
    "ALLOWED_ERROR_S = 2\n",
    "MIN_HEIGHT = 0.05\n",
    "\n",
    "TIME_RES = 0.5342  # duration of each spectrogram pixel in seconds\n",
    "FREQ_RES = 0.9375  # f of each spectrogram pixel in Hz\n",
    "HEIGHT = 128\n",
    "\n",
    "device = \"cuda\"  # if there is a GPU and CUDA is installed, device can be set to \"cuda\" instead\n",
    "model_det.to(device)\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def process_and_make_spec(idx, start, STEP, DELTA, end, sampling_f, HEIGHT):\n",
    "    seg_start = start + idx * STEP\n",
    "    seg_end = min(end, seg_start + DELTA)\n",
    "    manager = get_manager_for_worker()\n",
    "    data = manager.get_segment(seg_start, seg_end)\n",
    "    if len(data) / sampling_f <= 1:\n",
    "        return None\n",
    "\n",
    "    spec = make_spectrogram(data, manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "    if spec.shape[0] > HEIGHT:\n",
    "        spec = spec[-HEIGHT:]\n",
    "    elif spec.shape[0] < HEIGHT:\n",
    "        spec = np.pad(spec, ((HEIGHT - spec.shape[0], 0), (0, 0)), 'constant')\n",
    "    return (seg_start, spec[np.newaxis, :, :])\n",
    "\n",
    "_manager_cache = {}\n",
    "\n",
    "def get_manager_for_worker():\n",
    "    pid = os.getpid()\n",
    "    if pid not in _manager_cache:\n",
    "        if \"43\" in global_station.name:\n",
    "            global_station.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "        global_station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "        _manager_cache[pid] = global_station.get_manager()  # global_station est dÃ©fini par le process parent\n",
    "    return _manager_cache[pid]\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Starting detection on {station.name}\")\n",
    "    global_station = station  # pour les workers\n",
    "    if \"43\" in station.name:\n",
    "        station.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "    station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "    manager = station.get_manager()\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\"\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start) / STEP))\n",
    "    start_idx = 0\n",
    "\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try: last_date = pickle.load(f)[0]\n",
    "                except EOFError: break\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    with open(out_file, \"ab\") as f_out, \\\n",
    "         ProcessPoolExecutor() as executor:\n",
    "\n",
    "        for i in tqdm(range(start_idx, steps, BATCH_SIZE)):\n",
    "            idxs = [i + j for j in range(BATCH_SIZE) if (i + j) < steps]\n",
    "            results = list(executor.map(\n",
    "                process_and_make_spec,\n",
    "                idxs,\n",
    "                [start]*len(idxs),\n",
    "                [STEP]*len(idxs),\n",
    "                [DELTA]*len(idxs),\n",
    "                [end]*len(idxs),\n",
    "                [manager.sampling_f]*len(idxs),\n",
    "                [HEIGHT]*len(idxs)\n",
    "            ))\n",
    "\n",
    "            results = [r for r in results if r is not None]\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            times_loaded, spectros = zip(*results)\n",
    "\n",
    "            try:\n",
    "                batch_tensor = np.stack(spectros)\n",
    "                preds = process_batch(batch_tensor, device, model_det)\n",
    "                pairs = zip(times_loaded, preds)\n",
    "            except ValueError:\n",
    "                pairs = []\n",
    "                for time_, spec in zip(times_loaded, spectros):\n",
    "                    try:\n",
    "                        spec_tensor = np.expand_dims(spec, 0)\n",
    "                        pred = process_batch(spec_tensor, device, model_det)[0]\n",
    "                        pairs.append((time_, pred))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur dans le traitement du spectro isolÃ© : {e}\")\n",
    "\n",
    "            for seg_start, pred in pairs:\n",
    "                t_res = DELTA.total_seconds() / pred.shape[0]\n",
    "                peaks = find_peaks(pred, height=0, distance=math.ceil(ALLOWED_ERROR_S / t_res), prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * t_res\n",
    "                for j, t in enumerate(time_s):\n",
    "                    if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT:\n",
    "                        date = seg_start + datetime.timedelta(seconds=t)\n",
    "                        prob = peaks[1][\"peak_heights\"][j]\n",
    "                        pickle.dump([date, prob.astype(np.float16)], f_out)"
   ],
   "id": "59a6cfccf2169b5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.signal.make_spectrogram import make_spectrogram\n",
    "from utils.detection.TiSSNet import TiSSNet, process_batch\n",
    "\n",
    "catalog_path = \"/media/plerolland/akoustik/MAHY\"\n",
    "tissnet_checkpoint = \"../../../../data/models/TiSSNet/torch_save_checked_2\"\n",
    "out_dir = \"../../../../data/detection/TiSSNet_raw/MAHY\"  # where files will be saved\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)  # create output directory if needed\n",
    "\n",
    "stations = StationsCatalog(catalog_path).filter_out_undated() # remove stations with no start / end dates\n",
    "\n",
    "model_det = TiSSNet()\n",
    "model_det.load_state_dict(torch.load(tissnet_checkpoint))\n",
    "\n",
    "DELTA = datetime.timedelta(seconds=3600)  # duration of segments that are given to TiSSNet\n",
    "OVERLAP = 0.02   # overlap between those segments (no link with STFT)\n",
    "STEP = (1 - OVERLAP) * DELTA\n",
    "batch_size = 1  # number of segments that are fed together to TiSSNet\n",
    "\n",
    "# parameters of peak finding (TiSSNet outputs 1 value per spectrogram time bin, we use a peak finding algorithm to save only the peaks)\n",
    "TISSNET_PROMINENCE = 0.05\n",
    "ALLOWED_ERROR_S = 5\n",
    "MIN_HEIGHT = 0.05\n",
    "\n",
    "TIME_RES = 0.5342  # duration of each spectrogram pixel in seconds\n",
    "FREQ_RES = 0.9375  # f of each spectrogram pixel in Hz\n",
    "\n",
    "device = \"cuda\"  # if there is a GPU and CUDA is installed, device can be set to \"cuda\" instead\n",
    "model_det.to(device)\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "HEIGHT = 128\n",
    "\n",
    "def process_and_make_spec(idx, start, STEP, DELTA, end, sampling_f, HEIGHT):\n",
    "    seg_start = start + idx * STEP\n",
    "    seg_end = min(end, seg_start + DELTA)\n",
    "    manager = get_manager_for_worker()\n",
    "    data = manager.get_segment(seg_start, seg_end)\n",
    "    if len(data) / sampling_f <= 1:\n",
    "        return None\n",
    "\n",
    "    spec = make_spectrogram(data, manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "    if spec.shape[0] > HEIGHT:\n",
    "        spec = spec[-HEIGHT:]\n",
    "    elif spec.shape[0] < HEIGHT:\n",
    "        spec = np.pad(spec, ((HEIGHT - spec.shape[0], 0), (0, 0)), 'constant')\n",
    "    return (seg_start, spec[np.newaxis, :, :])\n",
    "\n",
    "_manager_cache = {}\n",
    "\n",
    "def get_manager_for_worker():\n",
    "    pid = os.getpid()\n",
    "    if pid not in _manager_cache:\n",
    "        if \"43\" in global_station.name:\n",
    "            global_station.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "        global_station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "        _manager_cache[pid] = global_station.get_manager()  # global_station est dÃ©fini par le process parent\n",
    "    return _manager_cache[pid]\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Starting detection on {station.name}\")\n",
    "    global_station = station  # pour les workers\n",
    "    if \"43\" in station.name:\n",
    "        station.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "    station.other_kwargs[\"raw\"] = True  # read raw data (i.e. not drift-corrected)\n",
    "    manager = station.get_manager()\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\"\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start) / STEP))\n",
    "    start_idx = 0\n",
    "\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try: last_date = pickle.load(f)[0]\n",
    "                except EOFError: break\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    with open(out_file, \"ab\") as f_out, \\\n",
    "         ProcessPoolExecutor() as executor:\n",
    "\n",
    "        for i in tqdm(range(start_idx, steps, BATCH_SIZE)):\n",
    "            idxs = [i + j for j in range(BATCH_SIZE) if (i + j) < steps]\n",
    "            results = list(executor.map(\n",
    "                process_and_make_spec,\n",
    "                idxs,\n",
    "                [start]*len(idxs),\n",
    "                [STEP]*len(idxs),\n",
    "                [DELTA]*len(idxs),\n",
    "                [end]*len(idxs),\n",
    "                [manager.sampling_f]*len(idxs),\n",
    "                [HEIGHT]*len(idxs)\n",
    "            ))\n",
    "\n",
    "            results = [r for r in results if r is not None]\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            times_loaded, spectros = zip(*results)\n",
    "\n",
    "            try:\n",
    "                batch_tensor = np.stack(spectros)\n",
    "                preds = process_batch(batch_tensor, device, model_det)\n",
    "                pairs = zip(times_loaded, preds)\n",
    "            except ValueError:\n",
    "                pairs = []\n",
    "                for time_, spec in zip(times_loaded, spectros):\n",
    "                    try:\n",
    "                        spec_tensor = np.expand_dims(spec, 0)\n",
    "                        pred = process_batch(spec_tensor, device, model_det)[0]\n",
    "                        pairs.append((time_, pred))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur dans le traitement du spectro isolÃ© : {e}\")\n",
    "\n",
    "            for seg_start, pred in pairs:\n",
    "                t_res = DELTA.total_seconds() / pred.shape[0]\n",
    "                peaks = find_peaks(pred, height=0, distance=math.ceil(ALLOWED_ERROR_S / t_res), prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * t_res\n",
    "                for j, t in enumerate(time_s):\n",
    "                    if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT:\n",
    "                        date = seg_start + datetime.timedelta(seconds=t)\n",
    "                        prob = peaks[1][\"peak_heights\"][j]\n",
    "                        pickle.dump([date, prob.astype(np.float16)], f_out)"
   ],
   "id": "dc5241dffae1078d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36cc6eaa76bf0302",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_reading.sound_data.sound_file_manager import DatFilesManager\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "HEIGHT = 128\n",
    "\n",
    "def process_and_make_spec(idx, start, STEP, DELTA, end, sampling_f, HEIGHT):\n",
    "    seg_start = start + idx * STEP\n",
    "    seg_end = min(end, seg_start + DELTA)\n",
    "    manager = get_manager_for_worker()\n",
    "    print(seg_start, manager.dataset_start)\n",
    "    data = manager.get_segment(seg_start, seg_end)\n",
    "    if len(data) / sampling_f <= 1:\n",
    "        return None\n",
    "\n",
    "    spec = make_spectrogram(data, manager.sampling_f, t_res=TIME_RES, f_res=FREQ_RES, return_bins=False, normalize=True, vmin=-35, vmax=140).astype(np.float32)\n",
    "    if spec.shape[0] > HEIGHT:\n",
    "        spec = spec[-HEIGHT:]\n",
    "    elif spec.shape[0] < HEIGHT:\n",
    "        spec = np.pad(spec, ((HEIGHT - spec.shape[0], 0), (0, 0)), 'constant')\n",
    "    return (seg_start, spec[np.newaxis, :, :])\n",
    "\n",
    "_manager_cache = {}\n",
    "\n",
    "def get_manager_for_worker():\n",
    "    pid = os.getpid()\n",
    "    if pid not in _manager_cache:\n",
    "        _manager_cache[pid] = global_station.get_manager()  # global_station est dÃ©fini par le process parent\n",
    "    return _manager_cache[pid]\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Starting detection on {station.name}\")\n",
    "    global_station = station  # pour les workers\n",
    "    manager = station.get_manager()\n",
    "    out_file = f\"{out_dir}/{station.dataset}_{station.name}.pkl\"\n",
    "\n",
    "\n",
    "    start, end = manager.dataset_start, manager.dataset_end\n",
    "    steps = int(np.ceil((end - start) / STEP))\n",
    "    start_idx = 0\n",
    "\n",
    "    if Path(out_file).exists():\n",
    "        with open(out_file, \"rb\") as f:\n",
    "            while True:\n",
    "                try: last_date = pickle.load(f)[0]\n",
    "                except EOFError: break\n",
    "        start_idx = int(np.floor((last_date - start) / STEP))\n",
    "\n",
    "    with open(out_file, \"ab\") as f_out, \\\n",
    "         ProcessPoolExecutor() as executor:\n",
    "\n",
    "        for i in tqdm(range(start_idx, steps, BATCH_SIZE)):\n",
    "            idxs = [i + j for j in range(BATCH_SIZE) if (i + j) < steps]\n",
    "            results = list(executor.map(\n",
    "                process_and_make_spec,\n",
    "                idxs,\n",
    "                [start]*len(idxs),\n",
    "                [STEP]*len(idxs),\n",
    "                [DELTA]*len(idxs),\n",
    "                [end]*len(idxs),\n",
    "                [manager.sampling_f]*len(idxs),\n",
    "                [HEIGHT]*len(idxs)\n",
    "            ))\n",
    "\n",
    "            results = [r for r in results if r is not None]\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            times_loaded, spectros = zip(*results)\n",
    "\n",
    "            try:\n",
    "                batch_tensor = np.stack(spectros)\n",
    "                preds = process_batch(batch_tensor, device, model_det)\n",
    "                pairs = zip(times_loaded, preds)\n",
    "            except ValueError:\n",
    "                pairs = []\n",
    "                for time_, spec in zip(times_loaded, spectros):\n",
    "                    try:\n",
    "                        spec_tensor = np.expand_dims(spec, 0)\n",
    "                        pred = process_batch(spec_tensor, device, model_det)[0]\n",
    "                        pairs.append((time_, pred))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur dans le traitement du spectro isolÃ© : {e}\")\n",
    "\n",
    "            for seg_start, pred in pairs:\n",
    "                t_res = DELTA.total_seconds() / pred.shape[0]\n",
    "                peaks = find_peaks(pred, height=0, distance=math.ceil(ALLOWED_ERROR_S / t_res), prominence=TISSNET_PROMINENCE)\n",
    "                time_s = peaks[0] * t_res\n",
    "                for j, t in enumerate(time_s):\n",
    "                    if peaks[1][\"peak_heights\"][j] > MIN_HEIGHT:\n",
    "                        date = seg_start + datetime.timedelta(seconds=t)\n",
    "                        prob = peaks[1][\"peak_heights\"][j]\n",
    "                        pickle.dump([date, prob.astype(np.float16)], f_out)"
   ],
   "id": "54f0390f3b7e3d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Take a look at the results",
   "id": "9f5a9a184ef2087a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "detection_file = f\"{out_dir}/MAHY0_MAHY01.pkl\"\n",
    "d = []\n",
    "with open(detection_file, \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            d.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "print(f\"{len(detection_file)} detections found\")\n",
    "\n",
    "\n",
    "dates_plot = np.array(d)[:,0]\n",
    "offset = 1\n",
    "df = pd.DataFrame({'date': dates_plot})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "counts = df.resample('1H', on='date').size().asfreq('1H', fill_value=0)\n",
    "\n",
    "sns.barplot(x=counts.index.strftime(\"%Hh\"), y=counts.values)\n",
    "plt.title(f\"Hourly detections from {dates_plot[0].day:02d}/{dates_plot[0].month:02d}/{dates_plot[0].year}\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of events\")"
   ],
   "id": "7be9c377810e6a4c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
