{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.detection.association_geodesic import squarize\n",
    "from utils.detection.association_geodesic_3D import compute_candidates, update_valid_grid, update_results, load_detections, compute_grids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameters initialization",
   "id": "16d4f579fa844d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# paths\n",
    "CATALOG_PATH = \"/media/plerolland/akoustik\"\n",
    "dataset = \"MAHY4\"\n",
    "DETECTIONS_DIR = f\"../../../../data/detection/TiSSNet_Pn_OBS-fixed/{dataset}\"\n",
    "SOUND_MODEL_PATH = f\"../../../../data/sound_model\"\n",
    "\n",
    "# Detections loading parameters\n",
    "MIN_P_TISSNET_PRIMARY = 0.3  # min probability of browsed detections\n",
    "MIN_P_TISSNET_SECONDARY = 0.15  # min probability of detections that can be associated with the browsed one\n",
    "MERGE_DELTA_S = 5 # threshold below which we consider two events should be merged\n",
    "MERGE_DELTA = datetime.timedelta(seconds=MERGE_DELTA_S)\n",
    "\n",
    "REQ_CLOSEST_STATIONS = 0  # The REQ_CLOSEST_STATIONS th closest stations will be required for an association to be valid\n",
    "\n",
    "# sound model definition\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated().by_dataset(dataset)\n",
    "seismic_paths = glob2.glob(\"../../../../data/MAHY/loc_3D/*.npz\")\n",
    "\n",
    "# association running parameters\n",
    "SAVE_PATH_ROOT = None  # change this to save the grids as figures, leave at None by default"
   ],
   "id": "d4118f9c7f8711d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detections processing",
   "id": "46a628c68bea2433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DETECTIONS_DIR_NAME = DETECTIONS_DIR.split(\"/\")[-1]\n",
    "\n",
    "Path(f\"{DETECTIONS_DIR}/cache\").mkdir(parents=True, exist_ok=True)\n",
    "DET_PATH = f\"{DETECTIONS_DIR}/cache/detections_{MIN_P_TISSNET_SECONDARY}_{MERGE_DELTA_S}.pkl\"\n",
    "if not Path(DET_PATH).exists():\n",
    "    STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "    det_files = [f for f in glob2.glob(DETECTIONS_DIR + \"/*.pkl\") if Path(f).is_file()]\n",
    "    DETECTIONS = load_detections(det_files, STATIONS, MIN_P_TISSNET_SECONDARY, merge_delta=datetime.timedelta(seconds=MERGE_DELTA_S))\n",
    "    with open(DET_PATH, \"wb\") as f:\n",
    "        pickle.dump((DETECTIONS), f)\n",
    "else:\n",
    "    with open(DET_PATH, \"rb\") as f:\n",
    "        DETECTIONS = pickle.load(f)\n",
    "\n",
    "idx_det = 0\n",
    "IDX_TO_DET = {}\n",
    "for idx, s in enumerate(DETECTIONS.keys()):\n",
    "    s.idx = idx  # indexes to store efficiently the associations\n",
    "    DETECTIONS[s] = list(DETECTIONS[s])\n",
    "    for i in range(len(DETECTIONS[s])):\n",
    "        DETECTIONS[s][i] = np.concatenate((DETECTIONS[s][i], [idx_det]))\n",
    "        IDX_TO_DET[idx_det] = DETECTIONS[s][i]\n",
    "        idx_det += 1\n",
    "    DETECTIONS[s] = np.array(DETECTIONS[s])\n",
    "DETECTION_IDXS = np.array(list(range(idx_det)))\n",
    "\n",
    "STATIONS = [s for s in DETECTIONS.keys()]\n",
    "for i in range(len(STATIONS)):\n",
    "    STATIONS[i].idx = i\n",
    "FIRSTS_DETECTIONS = {s : DETECTIONS[s][0,0] for s in STATIONS}\n",
    "LASTS_DETECTIONS = {s : DETECTIONS[s][-1,0] for s in STATIONS}\n",
    "\n",
    "DETECTIONS_MERGED = np.concatenate([[(det[0], det[1], det[2], s) for det in DETECTIONS[s]] for s in STATIONS])\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[DETECTIONS_MERGED[:, 1] > MIN_P_TISSNET_PRIMARY]\n",
    "DETECTIONS_MERGED = DETECTIONS_MERGED[np.argsort(DETECTIONS_MERGED[:, 1])][::-1]"
   ],
   "id": "2d60a698b6e577f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grid computation",
   "id": "40ca0d7a057636e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BOUNDS = [(1_000,100_000), (-13.4,-12.4), (45.25,46.25)]\n",
    "GRID_SIZES = [100, 100]\n",
    "PICK_UNCERTAINTY = 0.1\n",
    "PICK_UNCERTAINTY = 1\n",
    "MAX_CLOCK_DRIFT = 0.1\n",
    "GEOMETRICAL = 0.1\n",
    "\n",
    "GRID_PATH = f\"{DETECTIONS_DIR}/cache/grids_{BOUNDS[0][0]}_{BOUNDS[0][1]}_{BOUNDS[1][0]}_{BOUNDS[1][1]}_{BOUNDS[2][0]}_{BOUNDS[2][1]}_{GRID_SIZES[0]}_{GRID_SIZES[1]}_{PICK_UNCERTAINTY}_{MAX_CLOCK_DRIFT}.pkl\"\n",
    "\n",
    "if not Path(GRID_PATH).exists():\n",
    "    GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTY, LATS, DEPTHS, TRAVEL_TIMES = compute_grids(BOUNDS, GRID_SIZES, STATIONS, seismic_paths, pick_uncertainty=PICK_UNCERTAINTY, max_clock_drift=MAX_CLOCK_DRIFT, geometrical_uncertainty=GEOMETRICAL)\n",
    "    with open(GRID_PATH, \"wb\") as f:\n",
    "        pickle.dump((GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTY, LATS, DEPTHS, TRAVEL_TIMES), f)\n",
    "else:\n",
    "    with open(GRID_PATH, \"rb\") as f:\n",
    "        GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTY, LATS, DEPTHS, TRAVEL_TIMES = pickle.load(f)\n",
    "GRID_TO_COORDS = np.array(GRID_TO_COORDS)"
   ],
   "id": "918fcc1714146cd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "d, s1, s2 = DEPTHS[80], STATIONS[0], STATIONS[1]\n",
    "\n",
    "depth_mask = GRID_TO_COORDS[:,0]==d\n",
    "weights = TDoA[s1][s2][depth_mask]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sq = squarize(GRID_TO_COORDS[depth_mask,1:], weights, BOUNDS[1], BOUNDS[2], size=1000)\n",
    "\n",
    "im = ax.imshow(sq[::-1], cmap=\"seismic\",extent=(BOUNDS[2][0], BOUNDS[2][-1], BOUNDS[1][0], BOUNDS[1][-1]))\n",
    "xticks = np.arange(np.floor(BOUNDS[2][0]/0.25)*0.25, np.ceil(BOUNDS[2][-1]/0.25)*0.25 + 0.25, 0.25)\n",
    "yticks = np.arange(np.floor(BOUNDS[1][0]/0.25)*0.25, np.ceil(BOUNDS[1][-1]/0.25)*0.25 + 0.25, 0.25)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "for s_ in STATIONS:\n",
    "    p = s_.get_pos()\n",
    "\n",
    "    if p[0] > BOUNDS[1][1] or p[0] < BOUNDS[1][0] or p[1] > BOUNDS[2][1] or p[1] < BOUNDS[2][0]:\n",
    "        print(f\"Station {s_.name} out of bounds\")\n",
    "        continue\n",
    "    ax.plot(p[1], p[0], 'yx', alpha=0.75, markersize=10, markeredgewidth=3)\n",
    "    if \"3\" in s_.name:\n",
    "        ax.annotate(s_.name, xy=(p[1], p[0]), xytext=(p[1]-4.5*(BOUNDS[2][1]-BOUNDS[2][0])/30, p[0]+(BOUNDS[1][1]-BOUNDS[1][0])/50), textcoords=\"data\", color='y', alpha=0.9, weight='bold')\n",
    "    else:\n",
    "        ax.annotate(s_.name, xy=(p[1], p[0]), xytext=(p[1]-(BOUNDS[2][1]-BOUNDS[2][0])/30, p[0]+(BOUNDS[1][1]-BOUNDS[1][0])/50), textcoords=\"data\", color='y', alpha=0.9, weight='bold')\n",
    "cbar = plt.colorbar(im,fraction=0.0415, pad=0.04)\n",
    "ax.set_xlabel(\"lon (째)\")\n",
    "ax.set_ylabel(\"lat (째)\")\n",
    "plt.title(f\"Stations {s1.name}-{s2.name} (depth = {d/1_000} km)\")\n",
    "plt.savefig(f'{DETECTIONS_DIR}/figures/grids_{BOUNDS[0][0]}_{BOUNDS[0][1]}_{BOUNDS[1][0]}_{BOUNDS[1][1]}_{BOUNDS[2][0]}_{BOUNDS[2][1]}_{GRID_SIZES[0]}_{GRID_SIZES[1]}_{PICK_UNCERTAINTY}_{MAX_CLOCK_DRIFT}_xy.png', bbox_inches='tight')"
   ],
   "id": "16ba1cddec5cdc07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lat, s1, s2 = LATS[len(LATS)//2], STATIONS[0], STATIONS[1]\n",
    "\n",
    "lat_mask = GRID_TO_COORDS[:,1]==lat\n",
    "weights = TDoA[s1][s2][lat_mask]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sq = squarize(GRID_TO_COORDS[lat_mask][:,[0,2]], weights, BOUNDS[0], BOUNDS[2], size=1000)\n",
    "\n",
    "im = ax.imshow(sq, cmap=\"seismic\", aspect=\"auto\", extent=(BOUNDS[2][0], BOUNDS[2][-1], -BOUNDS[0][1]/1_000, -BOUNDS[0][0]/1_000))\n",
    "xticks = np.arange(np.floor(BOUNDS[2][0]/0.25)*0.25, np.ceil(BOUNDS[2][-1]/0.25)*0.25 + 0.25, 0.25)\n",
    "yticks = np.arange(np.floor(-BOUNDS[0][1]/5_000)*5, np.ceil(-BOUNDS[0][0]/5_000)*5 + 5, 5)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "\n",
    "cbar = plt.colorbar(im,fraction=0.0415, pad=0.04)\n",
    "ax.set_xlabel(\"lon (째)\")\n",
    "ax.set_ylabel(\"depth (km)\")\n",
    "plt.title(f\"Stations {s1.name}-{s2.name} (lat = {lat}째)\")\n",
    "plt.savefig(f'{DETECTIONS_DIR}/figures/grids_{BOUNDS[0][0]}_{BOUNDS[0][1]}_{BOUNDS[1][0]}_{BOUNDS[1][1]}_{BOUNDS[2][0]}_{BOUNDS[2][1]}_{GRID_SIZES[0]}_{GRID_SIZES[1]}_{PICK_UNCERTAINTY}_{MAX_CLOCK_DRIFT}_xz.png', bbox_inches='tight')"
   ],
   "id": "cedbc603678c842d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Association\n",
    "(note: parallelize this with e.g. ProcessPoolExecutor for large datasets)"
   ],
   "id": "5b2de7d7e967756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"starting association\")\n",
    "MIN_ASSOCIATION_SIZE = 3\n",
    "ASSOCIATION_RECORD_TOLERANCE = 0\n",
    "max_reached_per_det = {det_idx: MIN_ASSOCIATION_SIZE+ASSOCIATION_RECORD_TOLERANCE for det_idx in DETECTION_IDXS}\n",
    "\n",
    "\n",
    "already_examined = set()\n",
    "\n",
    "def process_detection(arg):\n",
    "    detection, already_examined, max_reached_per_det = arg\n",
    "    max_reached_per_det_modifications = {}\n",
    "    local_association = []\n",
    "    date1, p1, idx_det1, s1 = detection\n",
    "\n",
    "    # list all other stations and sort them by distance from s1\n",
    "    other_stations = np.array([s2 for s2 in STATIONS if s2 != s1\n",
    "                               and date1 + datetime.timedelta(days=1) > FIRSTS_DETECTIONS[s2]\n",
    "                               and date1 - datetime.timedelta(days=1) < LASTS_DETECTIONS[s2]])\n",
    "    other_stations = other_stations[np.argsort([MAX_TDoA[s1][s2] for s2 in other_stations])]\n",
    "\n",
    "    # given the detection date1 occurred on station s1, list all the detections of other stations that may be generated by the same source event\n",
    "    current_association = {s1:(date1, idx_det1)}\n",
    "    candidates = compute_candidates(other_stations, current_association, DETECTIONS, MAX_TDoA, MERGE_DELTA_S)\n",
    "\n",
    "    # update the list of other stations to only include the ones having at least a candidate detection\n",
    "    other_stations = [s for s in other_stations if len(candidates[s]) > 0]\n",
    "\n",
    "    # define the recursive browsing function (that is responsible for browsing the search space of associations for s1-date1)\n",
    "    def backtrack(station_index, current_association, valid_grid, associations):\n",
    "        if station_index == len(other_stations):\n",
    "            return\n",
    "        station = other_stations[station_index]\n",
    "\n",
    "        candidates = compute_candidates([station], current_association, DETECTIONS, MAX_TDoA, MERGE_DELTA_S)\n",
    "        probabilities = [DETECTIONS[station][idx][1] for idx in candidates[station]]\n",
    "        candidates[station] = np.array(candidates[station])[np.argsort(probabilities)][::-1][:10]\n",
    "        for idx in candidates[station]:\n",
    "            date, p, idx_det = DETECTIONS[station][idx]\n",
    "\n",
    "            if date in already_examined:\n",
    "                # the det was already browsed as main\n",
    "                continue\n",
    "            if len(other_stations) < max_reached_per_det[idx_det] - ASSOCIATION_RECORD_TOLERANCE - 1:\n",
    "            # the det already belongs to an association larger that what we could have here\n",
    "                continue\n",
    "\n",
    "            valid_grid_new, dg_new = update_valid_grid(current_association, valid_grid, station, date, TDoA, TDoA_UNCERTAINTY)\n",
    "\n",
    "            valid_points_new = np.argwhere(valid_grid_new)[:, 0]\n",
    "\n",
    "            if len(valid_points_new) > 0:\n",
    "                current_association[station] = (date, idx_det)\n",
    "\n",
    "                if np.all([len(current_association) >= max_reached_per_det[idx] - ASSOCIATION_RECORD_TOLERANCE for _, idx in\n",
    "                       current_association.values()]):\n",
    "                    update_results(current_association, valid_points_new, local_association, TDoA, TDoA_UNCERTAINTY)\n",
    "                    for _, idx in current_association.values():\n",
    "                        if len(current_association) > max_reached_per_det[idx]:\n",
    "                            max_reached_per_det[idx] = len(current_association)\n",
    "                            max_reached_per_det_modifications[idx] = len(current_association)\n",
    "                backtrack(station_index + 1, current_association, valid_grid_new, associations)\n",
    "                del current_association[station]\n",
    "        # also try without self\n",
    "        if station_index >= REQ_CLOSEST_STATIONS:\n",
    "            backtrack(station_index + 1, current_association, valid_grid, associations)\n",
    "        return\n",
    "\n",
    "    if len(other_stations) >= max_reached_per_det[idx_det1]-ASSOCIATION_RECORD_TOLERANCE - 1:\n",
    "        # we only browse other stations if we can make at least a trio\n",
    "        backtrack(0, current_association, None, associations)\n",
    "    return local_association, max_reached_per_det_modifications\n",
    "\n",
    "\n",
    "frac = 0.1\n",
    "n_chunks = math.ceil(1/frac)\n",
    "chunk_size = len(DETECTIONS_MERGED) // n_chunks\n",
    "chunks = [DETECTIONS_MERGED[i * chunk_size : (i + 1) * chunk_size] for i in range(n_chunks-1)]\n",
    "chunks.append(DETECTIONS_MERGED[9 * chunk_size :])\n",
    "\n",
    "\n",
    "# main part (note: process parallelization is a very efficient solution in case needed)\n",
    "for i in range(len(chunks)):\n",
    "    fname = f\"{DETECTIONS_DIR}/cache/associations_{PICK_UNCERTAINTY}_{MIN_ASSOCIATION_SIZE}_{i*frac:.02f}.pkl\"\n",
    "    if Path(fname).exists():\n",
    "        continue\n",
    "    associations = []\n",
    "    for det in tqdm(chunks[i]):\n",
    "        local_association, max_reached_per_det_modifications = process_detection((det, already_examined, max_reached_per_det))\n",
    "        already_examined.add(det[0])\n",
    "        associations.extend(local_association)\n",
    "        for i, v in max_reached_per_det_modifications.items():\n",
    "            max_reached_per_det[i] = max(max_reached_per_det[i], max_reached_per_det_modifications[i])\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(associations, f)"
   ],
   "id": "4548f27d219764c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(associations))",
   "id": "3858da52bdda322c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Associations plot",
   "id": "b25af14e5ece0fe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_per_coord = [0 for i in range(len(GRID_TO_COORDS))]\n",
    "\n",
    "association_files = glob2.glob(f\"{DETECTIONS_DIR}/cache/associations_{PICK_UNCERTAINTY}_{MIN_ASSOCIATION_SIZE}_*.pkl\")\n",
    "for file in association_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        associations = pickle.load(f)\n",
    "    for association in tqdm(associations):\n",
    "        detections, valid_points = association\n",
    "        if len(detections) < 4:\n",
    "            continue\n",
    "        for i in valid_points:\n",
    "            nb_per_coord[i] += 1\n",
    "\n",
    "nb_per_coord = np.array(nb_per_coord)"
   ],
   "id": "ddba07738ff04e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "mask = nb_per_coord > 50\n",
    "\n",
    "p = ax.scatter(GRID_TO_COORDS[mask, 2], GRID_TO_COORDS[mask, 1], -GRID_TO_COORDS[mask, 0]/1_000, c=nb_per_coord[mask], cmap='inferno', s=10)\n",
    "#df = pd.DataFrame({'x': GRID_TO_COORDS[mask, 2], 'y': GRID_TO_COORDS[mask, 1], 'z': -GRID_TO_COORDS[mask, 0]/1_000, 'val': nb_per_coord[mask]})\n",
    "#fig = px.scatter_3d(df, x='x', y='y', z='z', color='val', opacity=1.0, size_max=10)\n",
    "\n",
    "plt.xlim(45.5,45.7)\n",
    "plt.ylim(-12.8,-12.6)\n",
    "\n",
    "fig.colorbar(p, ax=ax, label='nb_per_coord')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3aaa05b41f4c3282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ax.view_init(elev=15, azim=240)\n",
    "\n",
    "mask = nb_per_coord > 25\n",
    "sc = ax.scatter(GRID_TO_COORDS[mask, 2], GRID_TO_COORDS[mask, 1], -GRID_TO_COORDS[mask, 0] / 1_000, c=nb_per_coord[mask], cmap='plasma', s=10, alpha=0.9)\n",
    "\n",
    "ax.set_xlabel(\"lon (째)\", fontsize=10)\n",
    "ax.set_ylabel(\"lat (째)\", fontsize=10)\n",
    "ax.set_zlabel(\"depth (km)\", fontsize=10)\n",
    "ax.grid(True)\n",
    "ax.tick_params(axis='both', labelsize=8)     # X et Y\n",
    "ax.tick_params(axis='z', labelsize=8)\n",
    "\n",
    "cb = fig.colorbar(sc, ax=ax, shrink=0.6)\n",
    "cb.set_label(\"Number of associations\")\n",
    "\n",
    "#plt.savefig(f'{DETECTIONS_DIR}/figures/proj_3d.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "id": "e15d52b26757ce0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob2\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "dep_bounds = [30_000, 60_000]\n",
    "dep_bounds = [35_000, 45_000]\n",
    "\n",
    "coords_2d = GRID_TO_COORDS[GRID_TO_COORDS[:, 0] == DEPTHS[0], 1:]\n",
    "coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_2d)}\n",
    "\n",
    "nb_per_coord = {n: Counter() for n in range(3, 5)}\n",
    "\n",
    "association_files = glob2.glob(f\"{DETECTIONS_DIR}/cache/associations_{PICK_UNCERTAINTY}_{MIN_ASSOCIATION_SIZE}_*.pkl\")\n",
    "for file in association_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        associations = pickle.load(f)\n",
    "    for detections, valid_points in tqdm(associations):\n",
    "        valid_coords = GRID_TO_COORDS[valid_points]\n",
    "        mask = (valid_coords[:, 0] > dep_bounds[0]) & (valid_coords[:, 0] < dep_bounds[1])\n",
    "        filtered_coords = valid_coords[mask]\n",
    "\n",
    "        for coord in filtered_coords[:, 1:]:\n",
    "            idx = coord_to_idx.get(tuple(coord))\n",
    "            if idx is not None:\n",
    "                nb_per_coord[len(detections)][idx] += 1"
   ],
   "id": "db3520792d9f33b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "min_size_display = 4\n",
    "\n",
    "log = False\n",
    "weights = np.array([np.sum([nb_per_coord[n][i] for n in range(min_size_display,5)]) for i in range(len(coords_2d))])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10.5,14))\n",
    "sq = squarize(coords_2d, weights, BOUNDS[1], BOUNDS[2], size=1000)\n",
    "if log:\n",
    "    sq[sq<1] = 1\n",
    "    sq = np.log10(sq)\n",
    "#sq[sq<1.5] = 1.5\n",
    "im = ax.imshow(sq[::-1], cmap=\"inferno\",extent=(BOUNDS[2][0], BOUNDS[2][-1], BOUNDS[1][0], BOUNDS[1][-1]))\n",
    "xticks = np.arange(np.floor(BOUNDS[2][0]/0.1)*0.1, np.ceil(BOUNDS[2][-1]/0.1)*0.1 + 0.1, 0.1)\n",
    "yticks = np.arange(np.floor(BOUNDS[1][0]/0.1)*0.1, np.ceil(BOUNDS[1][-1]/0.1)*0.1 + 0.1, 0.1)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "for s_ in STATIONS:\n",
    "    p = s_.get_pos()\n",
    "\n",
    "    if p[0] > BOUNDS[1][1] or p[0] < BOUNDS[1][0] or p[1] > BOUNDS[2][1] or p[1] < BOUNDS[2][0]:\n",
    "        print(f\"Station {s_.name} out of bounds\")\n",
    "        continue\n",
    "    ax.plot(p[1], p[0], 'yx', alpha=0.75, markersize=10, markeredgewidth=3)\n",
    "    ax.annotate(s_.name, xy=(p[1], p[0]), xytext=(p[1]-(BOUNDS[2][1]-BOUNDS[2][0])/30, p[0]+(BOUNDS[1][1]-BOUNDS[1][0])/50), textcoords=\"data\", color='y', alpha=0.9, weight='bold')\n",
    "cbar = plt.colorbar(im,fraction=0.0415, pad=0.04)\n",
    "cbar.set_label(f'Counts of resulting associations{\" (log)\" if log else \"\"}', rotation=270, labelpad=20)\n",
    "ax.set_xlabel(\"lon (째)\")\n",
    "ax.set_ylabel(\"lat (째)\")\n",
    "plt.tight_layout()\n",
    "Path(f\"{DETECTIONS_DIR}/figures\").mkdir(exist_ok=True)\n",
    "plt.savefig(f'{DETECTIONS_DIR}/figures/_min-{min_size_display}_{dep_bounds[0]/1_000:.0f}-{dep_bounds[1]/1_000:.0f}km_{\"log\" if log else \"\"}.png', bbox_inches='tight')"
   ],
   "id": "f29385bcfcb090d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from skimage import measure\n",
    "\n",
    "threshold = 1500 # MAHY3\n",
    "threshold = 4000 # MAHY2 (note: associations of size 3)\n",
    "threshold = 2000 # MAHY1\n",
    "threshold = 6000 # MAHY0\n",
    "threshold = 350 # MAHY4\n",
    "lat_bounds, lon_bounds = BOUNDS[1], BOUNDS[2]\n",
    "\n",
    "contours = measure.find_contours(sq[::-1], threshold)\n",
    "\n",
    "geoms = []\n",
    "for contour in contours:\n",
    "    if len(contour) < 3:\n",
    "        continue\n",
    "    lon_vals = np.linspace(lon_bounds[0], lon_bounds[1], sq.shape[1])\n",
    "    lat_vals = np.linspace(lat_bounds[0], lat_bounds[1], sq.shape[0])[::-1]\n",
    "    coords = [(lon_vals[int(x)], lat_vals[int(y)]) for y, x in contour]\n",
    "    geoms.append(Polygon(coords))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(geometry=geoms, crs=\"EPSG:4326\")\n",
    "gdf.to_file(f\"{DETECTIONS_DIR}/figures/contours.geojson\", driver=\"GeoJSON\")"
   ],
   "id": "64def051ba62865",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DELTA = pd.Timedelta(seconds=15)\n",
    "S_df = pd.read_csv(\n",
    "    \"../../../../data/MAHY/lavayssiere_and_public.csv\", header=None, names=[\"date\",\"lat\",\"lon\",\"depth\",\"mb\"], parse_dates=[\"date\"]\n",
    ")\n",
    "S_df = S_df[(S_df['date'] >= np.min(DETECTIONS_MERGED[:,0]) - 5*DELTA) & (S_df['date'] <= np.max(DETECTIONS_MERGED[:,0]))]\n",
    "pts_df = []\n",
    "\n",
    "matched = {}\n",
    "\n",
    "association_files = glob2.glob(f\"{DETECTIONS_DIR}/cache/associations_{PICK_UNCERTAINTY}_{3}_*.pkl\")\n",
    "for file in association_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        associations = pickle.load(f)\n",
    "    for detections, valid_points in tqdm(associations):\n",
    "        coords = GRID_TO_COORDS[valid_points]\n",
    "        per_depth = [coords[coords[:,0]==d] for d in np.unique(coords[:,0])]\n",
    "        chosen_depth = np.argmax(len(p) for p in per_depth)\n",
    "        coords = np.mean(per_depth[chosen_depth], axis=0)\n",
    "\n",
    "        dates = np.array([IDX_TO_DET[idx][0] for idx in detections[:,1]])\n",
    "        date = dates[0] + np.mean(dates[0] - dates) - datetime.timedelta(seconds=12)\n",
    "\n",
    "        pts_df.append({\"date\":date, \"depth\":coords[0], \"lat\":coords[1], \"lon\":coords[2]})\n",
    "\n",
    "        candidates = S_df[(S_df['date'] >= date - DELTA) & \\\n",
    "       (S_df['date'] <= date + DELTA)]\n",
    "        for idx in candidates.index:\n",
    "            d_diff = np.sqrt(\n",
    "                (S_df['lat'][idx] - coords[1])**2+\n",
    "                (S_df['lon'][idx] - coords[2])**2+\n",
    "                ((S_df['depth'][idx] - coords[0]) / 111_000)**2)\n",
    "\n",
    "            t_diff = S_df['date'][idx] - date\n",
    "\n",
    "            matched.setdefault(idx, []).append((t_diff.total_seconds(), d_diff, detections, coords))\n",
    "\n",
    "n = 0\n",
    "S_df_matched = S_df.loc[list(matched.keys())].copy()\n",
    "for idx in matched.keys():\n",
    "    if len(matched[idx]) > 1:\n",
    "        longest = np.argmax([len(d[2]) for d in matched[idx]])\n",
    "        best = np.argmin([d[1] for d in matched[idx]])\n",
    "        if len(matched[idx][longest][2]) == 4:\n",
    "            n += 1\n",
    "            best = longest\n",
    "        matched[idx] = [matched[idx][best]]\n",
    "\n",
    "    h_dates = []\n",
    "    for si, di in matched[idx][0][2]:\n",
    "        s, (date, _, _) = STATIONS[si], IDX_TO_DET[di]\n",
    "        S_df_matched.loc[idx, s.name] = date\n",
    "        cell = np.argmin(np.sqrt((matched[idx][0][3][0]-GRID_TO_COORDS[:,0])**2 + (111_000*(matched[idx][0][3][1]-GRID_TO_COORDS[:,1]))**2 + (111_000*(matched[idx][0][3][2]-GRID_TO_COORDS[:,2]))**2))\n",
    "        h_dates.append(date - datetime.timedelta(seconds=TRAVEL_TIMES[s][cell]))\n",
    "    S_df_matched.loc[idx, \"h_date\"] = h_dates[0] + np.mean([hd - h_dates[0] for hd in h_dates])\n",
    "    S_df_matched.loc[idx, \"h_depth\"] = matched[idx][0][3][0]/1_000\n",
    "    S_df_matched.loc[idx, \"h_lat\"] = matched[idx][0][3][1]\n",
    "    S_df_matched.loc[idx, \"h_lon\"] = matched[idx][0][3][2]\n",
    "\n",
    "print(n, len(S_df_matched), len(S_df))\n",
    "\n",
    "h = dataset[-1]\n",
    "S_df_matched.to_csv(f'../../../../data/MAHY/loc_3D/twin-cat/{dataset}_OBS-fixed.csv', index=False, columns=[\"date\",\"h_date\",\"lat\",\"h_lat\",\"lon\",\"h_lon\",\"depth\",\"h_depth\",\"mb\",f\"MAHY{h}1\",f\"MAHY{h}2\",f\"MAHY{h}3\",f\"MAHY{h}4\"], float_format='%.3f')\n",
    "\n",
    "pts_df = pd.DataFrame(pts_df)\n",
    "pts_df.to_csv(f'../../../../data/MAHY/loc_3D/twin-cat/{dataset}_all_OBS-fixed.csv', index=False, columns=[\"date\",\"lat\",\"lon\",\"depth\"], float_format='%.3f')"
   ],
   "id": "34e85744c5536bcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MAHY4 : 1 - 194 326 532\n",
    "# MAHY3 : 1 - 403 629 1267\n",
    "# MAHY2 : 1 - 0 748 1697\n",
    "# MAHY1 : 1 - 405 609 1379\n",
    "# MAHY0 : 1 - 759 1120 1920"
   ],
   "id": "66531182e3e4b5e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MAHY 4 example:\n",
    "\n",
    "# no intercept\n",
    "# 0.2 : 86 322 532\n",
    "# 0.5 : 119 324 532\n",
    "# 1 : 165 325 532\n",
    "\n",
    "# intercept\n",
    "# 0.2 : 95 321 532\n",
    "# 0.5 : 147 323 532\n",
    "# 1 : 194 326 532"
   ],
   "id": "cfcc52f19065b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_shifts = sum([[v[0] for v in m] for m in matched.values()], [])\n",
    "plt.hist(all_shifts)\n",
    "plt.xlim(-15,15)\n",
    "\n",
    "plt.figure()\n",
    "all_shifts = sum([[v[1] for v in m] for m in matched.values()], [])\n",
    "plt.hist(all_shifts)"
   ],
   "id": "1c6ddcc760e6a1d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bb3abf5771b8e1fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
