{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.detection.association_geodesic import squarize\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats\n",
    "from utils.physics.geodesic.distance import distance_point_point\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "})\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "import math\n",
    "from numpy.linalg import LinAlgError\n",
    "import pandas as pd\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import GridSphericalSoundModel as GridSoundModel, MonthlyHomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association_geodesic import compute_candidates, update_valid_grid, update_results, load_detections, compute_grids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "})\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ],
   "id": "77f82e0058a95e16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STATIONS = StationsCatalog(\"/media/plerolland/akoustik/MAHY\").filter_out_undated().filter_out_unlocated()\n",
    "seismic_paths = glob2.glob(\"../../../../data/MAHY/loc_3D/*.npz\")\n",
    "acoustic_to_s = {}\n",
    "for s in STATIONS:\n",
    "    depth, bathy = s.other_kwargs[\"depth\"], s.other_kwargs[\"bathy\"]\n",
    "    under_hydro = bathy - depth\n",
    "    acoustic_to_s[s] = under_hydro / 1520\n",
    "\n",
    "interp_seismic = {}\n",
    "for seismic_propa_path in seismic_paths:\n",
    "    depth = float(seismic_propa_path.split(\"_\")[-1].split(\"m\")[0])\n",
    "    data = np.load(seismic_propa_path)\n",
    "    seismic_propagations = data[\"values\"]\n",
    "    seismic_propagations[(seismic_propagations < 10 ** -6) | (seismic_propagations > 10 ** 6)] = np.nan\n",
    "    seismic_depths = data[\"depths\"] * 1_000\n",
    "    seismic_distances = data[\"distances\"] * 1_000\n",
    "    interp_seismic[depth] = RegularGridInterpolator(\n",
    "        (seismic_depths, seismic_distances),\n",
    "        seismic_propagations,\n",
    "        bounds_error=False,  # allow extrapolation\n",
    "        fill_value=None)\n",
    "available_depths = np.array(list(interp_seismic.keys()))\n",
    "s_to_interp = {s: interp_seismic[available_depths[np.argmin(abs(available_depths - s.other_kwargs[\"bathy\"]))]]\n",
    "               for s in STATIONS}\n",
    "\n",
    "match_files = glob2.glob(\"../../../../data/MAHY/loc_3D/twin-cat/*_raw_OBS-fixed.csv\")\n",
    "match_files = glob2.glob(\"../../../../data/MAHY/loc_3D/twin-cat/*_raw.csv\")\n",
    "match = {}\n",
    "for f in match_files:\n",
    "    d = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "    match[d] = pd.read_csv(f, parse_dates=['date'] + [s.name for s in STATIONS.by_dataset(d)])"
   ],
   "id": "271537a43f4eb263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "r = np.sum([len(m) for m in match.values()])\n",
    "print(r)"
   ],
   "id": "a1d9c32fe8229296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TIMES",
   "id": "18c87236318ad4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "deltas = {}\n",
    "\n",
    "for dataset in match.keys():\n",
    "    for idx in tqdm(match[dataset].index):\n",
    "        a = match[dataset].loc[idx]\n",
    "        for s in STATIONS.by_dataset(dataset):\n",
    "            if not type(a[s.name]) == pd._libs.tslibs.nattype.NaTType:\n",
    "                dist = distance_point_point([a[\"lat\"],a[\"lon\"]], s.get_pos())\n",
    "                seismic_travel_path = s_to_interp[s]([[a[\"depth\"]*1_000, dist]])[0]\n",
    "                expected = seismic_travel_path + acoustic_to_s[s]\n",
    "                actual = a[s.name] - a[\"date\"]\n",
    "\n",
    "                deltas.setdefault(s, {})[a[\"date\"]] = actual.total_seconds() - expected"
   ],
   "id": "af411d51fbad42ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# version with no fixed intercept\n",
    "def dates_format(x, pos):\n",
    "    dt = mdates.num2date(x)\n",
    "    return r'\\shortstack{%s\\\\%s}' % (dt.strftime('%d/%m'), dt.strftime('%Y'))\n",
    "\n",
    "corrections = {}  # contains, for each station, the slope and intercept of a linear correction (starting from s.date_start)\n",
    "res_csv = \"\"\n",
    "\n",
    "for s in STATIONS:\n",
    "    if s.dataset not in match:\n",
    "        print(f\"Skipping station {s}\")\n",
    "        continue\n",
    "\n",
    "    values = np.array(list(deltas[s].values()))\n",
    "    mask = np.abs(values - np.mean(values)) < 1*np.std(values)\n",
    "    values = values[mask]\n",
    "    times = np.array(list(deltas[s].keys()))[mask]\n",
    "    values, times = values[np.argsort(times)], times[np.argsort(times)]\n",
    "    t_sec = [(t - s.date_start).total_seconds() for t in times]\n",
    "\n",
    "    (slope, intercept), cov = np.polyfit(t_sec, values, 1, cov=True)\n",
    "    var_slope, var_offset = np.diag(cov)\n",
    "    sigma_slope = np.sqrt(var_slope)\n",
    "    sigma_offset = np.sqrt(var_offset)\n",
    "    drift_ppm = slope * 1e6\n",
    "    ci_slope_ppm = 1.96 * sigma_slope * 1e6  # 95% CI for slope in ppm\n",
    "\n",
    "    #drift_fit = slope * np.array(t_sec) + intercept\n",
    "    #residuals = values - drift_fit\n",
    "    #diff_mean = values - np.mean(values)\n",
    "    #r_value = 1 - np.sum(residuals**2) / np.sum(diff_mean**2)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(t_sec, values)\n",
    "\n",
    "    corrections[s] = (slope, intercept, ci_slope_ppm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 3))\n",
    "    ax.scatter(times, values, marker='o', linestyle='-', alpha=0.6)\n",
    "    ax.plot(times, [intercept + slope * t for t in t_sec], 'r')\n",
    "    ax.set_title(f\"{s.name} : {intercept:.2f} + {slope*1e6:.3f}ppm (R² = {r_value**2:.2f}, uncertainty 95\\% = {ci_slope_ppm:.2f}ppm)\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"actual - expected (s)\")\n",
    "    ax.grid(True)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator(bymonth=range(1, 13)))\n",
    "    ax.xaxis.set_major_formatter(dates_format)\n",
    "\n",
    "    res_csv += f'{s.name},{intercept:.2f},{slope*1e6:.3f},{ci_slope_ppm:.3f}\\n'\n",
    "\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.savefig(f\"../../../../data/MAHY/figures/dérives/{s.name}_drift.pdf\",\n",
    "    dpi=500,\n",
    "    bbox_inches='tight',\n",
    "    pad_inches=0\n",
    ")\n",
    "\n",
    "with open(\"../../../../data/detection/TiSSNet_Pn_raw_repicked/corrections.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corrections, f)\n",
    "with open(\"../../../../data/detection/TiSSNet_Pn_raw_repicked/corrections.csv\", \"w\") as f:\n",
    "    f.write(res_csv)"
   ],
   "id": "4317edc5fb861071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fixed intercept\n",
    "\n",
    "corrections = {}  # contains, for each station, the slope and intercept of a linear correction (starting from s.date_start)\n",
    "res_csv = \"\"\n",
    "\n",
    "for i in range(1,5):\n",
    "    local_stations = []\n",
    "    time_series, time_series_s, value_series = {}, {}, {}\n",
    "\n",
    "    for s in STATIONS:\n",
    "        if s.name[-1] != str(i):\n",
    "            continue\n",
    "        if s.dataset not in match:\n",
    "            print(f\"Skipping station {s}\")\n",
    "            continue\n",
    "        local_stations.append(s)\n",
    "\n",
    "        values = np.array(list(deltas[s].values()))\n",
    "        mask = np.abs(values - np.mean(values)) < 1*np.std(values)\n",
    "        values = values[mask]\n",
    "        times = np.array(list(deltas[s].keys()))[mask]\n",
    "        value_series[s], time_series[s] = values[np.argsort(times)], times[np.argsort(times)]\n",
    "        time_series_s[s] = [(t - s.date_start).total_seconds() for t in time_series[s]]\n",
    "\n",
    "    total_points = sum(len(t) for t in time_series_s.values())\n",
    "    X = np.zeros((total_points, len(local_stations) + 1))\n",
    "    Y = np.zeros(total_points)\n",
    "    row = 0\n",
    "    for k, (t, y) in enumerate(zip(time_series_s.values(), value_series.values())):\n",
    "        n_points = len(t)\n",
    "        X[row:row+n_points, k] = t\n",
    "        X[row:row+n_points, -1] = 1\n",
    "        Y[row:row+n_points] = y\n",
    "        row += n_points\n",
    "    params, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
    "    n, p = X.shape\n",
    "    sigma2 = SS_res / (n - p)\n",
    "    cov_matrix = sigma2 * np.linalg.inv(X.T @ X)\n",
    "    std_errors = np.sqrt(np.diag(cov_matrix))[:-1]\n",
    "    uncertainties = 1.96 * std_errors * 1e6\n",
    "\n",
    "    # PLOT\n",
    "    intercept = params[-1]\n",
    "    for si, s in enumerate(local_stations):\n",
    "        slope = params[si]\n",
    "        y_pred_k = slope * np.array(time_series_s[s]) + intercept\n",
    "        ss_res_k = np.sum((value_series[s] - y_pred_k)**2)\n",
    "        ss_tot_k = np.sum((value_series[s] - np.mean(y))**2)\n",
    "        R2 = 1 - ss_res_k / ss_tot_k\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.scatter(time_series[s], value_series[s], marker='o', linestyle='-', alpha=0.6)\n",
    "        plt.plot(time_series[s], [intercept + slope * t for t in time_series_s[s]], 'r')\n",
    "        plt.title(f\"{s.name} : {intercept:.2f} + {slope*1e6:.3f}ppm (R² = {R2:.2f}, uncertainty 95\\% = {uncertainties[si]:.2f}ppm)\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"actual - expected (s)\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        res_csv += f'{s.name},{intercept:.2f},{slope*1e6:.3f},{uncertainties[si]:.3f}\\n'\n",
    "        corrections[s]=(slope, intercept)\n",
    "\n",
    "        plt.savefig(f\"../../../../data/MAHY/figures/drifts_fixed-intercept/{s.name}.png\", dpi=500, bbox_inches='tight')\n",
    "\n",
    "\n",
    "with open(\"../../../../data/detection/TiSSNet_Pn_raw_repicked/corrections_fixed-intercept.csv\", \"w\") as f:\n",
    "    f.write(res_csv)"
   ],
   "id": "62b8cd816a19c01c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e2fd106810841738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for s in STATIONS:\n",
    "    detections = []\n",
    "    with open(f\"../../../../data/detection/TiSSNet_Pn_raw_repicked/{s.dataset}/{s.dataset}_{s.name}.pkl\", \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                detections.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "    new_detections = []\n",
    "    for date, p in tqdm(detections[0]):\n",
    "        delta = corrections[s][1] + corrections[s][0] * (date-s.date_start).total_seconds()\n",
    "        new_detections.append((date - datetime.timedelta(seconds=delta), p))\n",
    "    out_dir = f\"../../../../data/detection/TiSSNet_Pn_raw_OBS-fixed/{s.dataset}\"\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(f\"{out_dir}/{s.dataset}_{s.name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(np.array(new_detections), f)"
   ],
   "id": "da9c11f5029dc31c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths_to_chg = [\"../../../../data/detection/TiSSNet_raw\",\"../../../../data/detection/i_TiSSNet_raw\"]\n",
    "\n",
    "for path_to_chg in paths_to_chg:\n",
    "    for s in STATIONS:\n",
    "        detections = []\n",
    "        with open(f\"{path_to_chg}/{s.dataset}/{s.dataset}_{s.name}.pkl\", \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    detections.append(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        new_detections = []\n",
    "        for date, p in tqdm(np.array(detections).reshape(-1,2)):\n",
    "            delta = corrections[s][0] * (date-s.date_start).total_seconds()\n",
    "            new_detections.append((date - datetime.timedelta(seconds=delta), p))\n",
    "        new_detections = np.array(new_detections)\n",
    "        out_dir = f\"{path_to_chg}_OBS-fixed/{s.dataset}\"\n",
    "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        with open(f\"{out_dir}/{s.dataset}_{s.name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(np.array(new_detections), f)"
   ],
   "id": "223ac8f4b1308272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Magnitudes",
   "id": "41942fbaa4af6038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mb = {}\n",
    "RL = {}\n",
    "\n",
    "DELTA = datetime.timedelta(seconds=1)\n",
    "\n",
    "for dataset in match.keys():\n",
    "    for idx in tqdm(match[dataset].index):\n",
    "        a = match[dataset].loc[idx]\n",
    "        for s in STATIONS.by_dataset(dataset):\n",
    "            if not type(a[s.name]) == pd._libs.tslibs.nattype.NaTType:\n",
    "                data = s.get_manager().get_segment(a[s.name] - DELTA, a[s.name] + DELTA)\n",
    "                v = np.max(np.square(data))\n",
    "\n",
    "                mb.setdefault(s, []).append(a[\"mb\"])\n",
    "                RL.setdefault(s, []).append(v)"
   ],
   "id": "8df289b21d156bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for s in STATIONS:\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(mb[s], np.log10(RL[s]))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.scatter(mb[s], np.log10(RL[s]), marker='o', linestyle='-', alpha=0.6)\n",
    "    plt.plot(mb[s], intercept + slope * np.array(mb[s]), 'r')\n",
    "\n",
    "    plt.title(f\"{s.name} : {intercept:.2f} + {slope*1e6:.3f}ppm (R² = {r_value**2:.2f})\")\n",
    "    plt.xlabel(\"mb\")\n",
    "    plt.ylabel(\"RL (db re 1 uPa)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "f21286d053e9a976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### mb completeness",
   "id": "51ae88c2483b9fe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "S_df = pd.read_csv(\n",
    "    \"../../../../data/MAHY/lavayssiere_and_public.csv\", header=None, names=[\"date\",\"lat\",\"lon\",\"depth\",\"mb\"], parse_dates=[\"date\"]\n",
    ")\n",
    "\n",
    "mb_detected = []\n",
    "for dataset in match.keys():\n",
    "    for idx in match[dataset].index:\n",
    "        mb_detected.append(match[dataset].loc[idx][\"mb\"])\n",
    "mb_detected = np.array(mb_detected)\n",
    "\n",
    "m_list, v_list = [], []\n",
    "for m in np.linspace(0,4,100):\n",
    "    nb_events = np.count_nonzero(S_df[\"mb\"] > m)\n",
    "    nb_events_detected = np.count_nonzero(mb_detected > m)\n",
    "\n",
    "    m_list.append(m)\n",
    "    v_list.append(100 * nb_events_detected / nb_events)\n",
    "\n",
    "plt.plot(m_list, v_list)"
   ],
   "id": "63b65ff9312075b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c77613708d69796",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
