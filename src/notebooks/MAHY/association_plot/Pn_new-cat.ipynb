{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.detection.association_geodesic import squarize\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "})\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "import math\n",
    "from numpy.linalg import LinAlgError\n",
    "import pandas as pd\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "from collections import Counter\n",
    "from utils.physics.geodesic.distance import distance_point_point\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.sound_model.spherical_sound_model import GridSphericalSoundModel as GridSoundModel, MonthlyHomogeneousSphericalSoundModel as HomogeneousSoundModel\n",
    "from utils.detection.association_geodesic import compute_candidates, update_valid_grid, update_results, load_detections, compute_grids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "# paths\n",
    "CATALOG_PATH = \"/media/plerolland/akoustik/MAHY\"\n",
    "Pn_DETECTIONS_DIR = f\"../../../../data/detection/TiSSNet_Pn_raw_OBS-fixed\"\n",
    "\n",
    "# sound model definition\n",
    "STATIONS = StationsCatalog(CATALOG_PATH).filter_out_undated().filter_out_unlocated()\n",
    "\n",
    "LAT_BOUNDS = [-13.4, -12.4]\n",
    "LON_BOUNDS = [45.25, 46.25]\n",
    "with open(\"../../../../data/detection/i_TiSSNet_raw_OBS-fixed/MAHY0/cache/grids_-13.4_-12.4_45.25_46.25_150_1_0.25_0.25.pkl\", \"rb\") as f:\n",
    "    GRID_TO_COORDS, TDoA, MAX_TDoA, TDoA_UNCERTAINTIES = pickle.load(f)\n",
    "GRID_TO_COORDS = np.array(GRID_TO_COORDS)\n",
    "Pn_BOUNDS = [(1_000,100_000), (-13.4,-12.4), (45.25,46.25)]\n",
    "with open(\"../../../../data/detection/TiSSNet_Pn_raw_OBS-fixed/MAHY0/cache/grids_1000_100000_-13.4_-12.4_45.25_46.25_100_100_1_0.1.pkl\", \"rb\") as f:\n",
    "    Pn_GRID_TO_COORDS, Pn_TDoA, Pn_MAX_TDoA, Pn_TDoA_UNCERTAINTIES, Pn_LATS, Pn_DEPTHS, Pn_TRAVEL_TIMES = pickle.load(f)\n",
    "Pn_GRID_TO_COORDS = np.array(Pn_GRID_TO_COORDS)\n",
    "\n",
    "\n",
    "seismic_paths = glob2.glob(\"../../../../data/MAHY/loc_3D/*.npz\")\n",
    "acoustic_to_s = {}\n",
    "for s in STATIONS:\n",
    "    depth, bathy = s.other_kwargs[\"depth\"], s.other_kwargs[\"bathy\"]\n",
    "    under_hydro = bathy - depth\n",
    "    acoustic_to_s[s] = under_hydro / 1520\n",
    "interp_seismic = {}\n",
    "for seismic_propa_path in seismic_paths:\n",
    "    depth = float(seismic_propa_path.split(\"_\")[-1].split(\"m\")[0])\n",
    "    data = np.load(seismic_propa_path)\n",
    "    seismic_propagations = data[\"values\"]\n",
    "    seismic_propagations[(seismic_propagations < 10 ** -6) | (seismic_propagations > 10 ** 6)] = np.nan\n",
    "    seismic_depths = data[\"depths\"] * 1_000\n",
    "    seismic_distances = data[\"distances\"] * 1_000\n",
    "    interp_seismic[depth] = RegularGridInterpolator(\n",
    "        (seismic_depths, seismic_distances),\n",
    "        seismic_propagations,\n",
    "        bounds_error=False,  # allow extrapolation\n",
    "        fill_value=None)\n",
    "available_depths = np.array(list(interp_seismic.keys()))\n",
    "s_to_interp = {s: interp_seismic[available_depths[np.argmin(abs(available_depths - s.other_kwargs[\"bathy\"]))]]\n",
    "               for s in STATIONS}\n",
    "\n",
    "clock_corrections = pd.read_csv(\"../../../../data/detection/TiSSNet_Pn_raw_repicked/corrections_fixed-intercept.csv\", names=[\"s\",\"intercept\",\"slope\",\"u\"], header=None).set_index('s')"
   ],
   "id": "724af32e9cfd9325",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_ASSOCIATION_SIZE = 3\n",
    "datasets = set([s.dataset for s in STATIONS])\n",
    "Pn_new_stations = {}\n",
    "Pn_idx_to_det = {}\n",
    "for dataset in datasets:\n",
    "    dets = glob2.glob(f\"{Pn_DETECTIONS_DIR}/{dataset}/cache/detections*.pkl\")[0]\n",
    "    with open(dets, \"rb\") as f:\n",
    "        Pn_DETECTIONS = pickle.load(f)\n",
    "    Pn_idx_det = 0\n",
    "    Pn_idx_to_det_local = {}\n",
    "    for idx, s in enumerate(Pn_DETECTIONS.keys()):\n",
    "        s.idx = idx  # indexes to store efficiently the associations\n",
    "        Pn_DETECTIONS[s] = list(Pn_DETECTIONS[s])\n",
    "        for i in range(len(Pn_DETECTIONS[s])):\n",
    "            Pn_DETECTIONS[s][i] = np.concatenate((Pn_DETECTIONS[s][i], [Pn_idx_det]))\n",
    "            Pn_idx_to_det_local[Pn_idx_det] = Pn_DETECTIONS[s][i]\n",
    "            Pn_idx_det += 1\n",
    "        Pn_DETECTIONS[s] = np.array(Pn_DETECTIONS[s])\n",
    "    Pn_new_stations[dataset] = list(Pn_DETECTIONS.keys())\n",
    "    Pn_idx_to_det[dataset] = Pn_idx_to_det_local\n",
    "\n",
    "STATIONS = Pn_new_stations"
   ],
   "id": "51ea9deddd3ef7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DELTA = datetime.timedelta(seconds=30)\n",
    "\n",
    "\n",
    "catalog = []\n",
    "for dataset in datasets:\n",
    "    association_files = glob2.glob(f\"{Pn_DETECTIONS_DIR}/{dataset}/cache/associations_{1}_{MIN_ASSOCIATION_SIZE}_*.pkl\")\n",
    "    for file in association_files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            associations = pickle.load(f)\n",
    "        for detections, valid_points in tqdm(associations):\n",
    "            valid_coords = Pn_GRID_TO_COORDS[valid_points[:,0].astype(np.int32)]\n",
    "            pos = np.nanmean(valid_coords, axis=0)\n",
    "            if np.any(np.isnan(pos)):\n",
    "                continue\n",
    "            catalog.append({\"depth\":pos[0],\"latitude\":pos[1], \"longitude\":pos[2]})\n",
    "\n",
    "            dates, dates_reception = [], []\n",
    "            d_h, d_v_hydro, d_v_seismo, RL = [], [], [], []\n",
    "            for si, di in detections:\n",
    "                s = STATIONS[dataset][si]\n",
    "                if \"43\" in s.name:\n",
    "                    s.path = \"/media/plerolland/akoustik/MAHY/MAHY4_fixed/MAHY43\"\n",
    "\n",
    "                s.other_kwargs[\"raw\"] = True\n",
    "                date = Pn_idx_to_det[dataset][di][0]\n",
    "\n",
    "                hydro_dep, local_bathy = s.other_kwargs[\"depth\"], s.other_kwargs[\"bathy\"]\n",
    "                d_h.append(distance_point_point([pos[1],pos[2]], s.get_pos()))\n",
    "                d_v_hydro.append(local_bathy-hydro_dep)\n",
    "                d_v_seismo.append(pos[0] - local_bathy)\n",
    "\n",
    "                dates_reception.append(date)\n",
    "                seismic_travel_path = s_to_interp[s]([[pos[0], d_h[-1]]])[0]\n",
    "                dates.append(date - datetime.timedelta(seconds=(seismic_travel_path + acoustic_to_s[s])))\n",
    "\n",
    "                c = clock_corrections.loc[s.name][\"intercept\"] + clock_corrections.loc[s.name][\"slope\"] * (date-s.date_start).total_seconds() * 10**-6\n",
    "                c = datetime.timedelta(seconds=c)\n",
    "                data = s.get_manager().get_segment(date+c,date+c+DELTA)\n",
    "                RL.append(np.log10(np.mean(data**2)))\n",
    "\n",
    "            date = dates[0] + np.mean(np.array(dates) - dates[0])\n",
    "            catalog[-1][\"d_v_seismo\"] = d_v_seismo\n",
    "            catalog[-1][\"d_v_hydro\"] = d_v_hydro\n",
    "            catalog[-1][\"d_h\"] = d_h\n",
    "            catalog[-1][\"RL\"] = RL\n",
    "            catalog[-1][\"dates\"] = dates\n",
    "            catalog[-1][\"dates_orig\"] = dates_reception\n",
    "            catalog[-1][\"date\"] = date\n",
    "            catalog[-1][\"n_stations\"] = len(detections)\n",
    "\n",
    "            #catalog[-1][\"mb\"] = -141.3/14.8 + SL / 14.8"
   ],
   "id": "71092fb6e194681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "whole_df = pd.DataFrame(catalog)\n",
    "whole_df.to_pickle(\"save_with-subsets.pkl\")\n",
    "print(len(whole_df))\n",
    "\n",
    "# remove associations that are subsets of other associations\n",
    "sets_dates = whole_df[\"dates_orig\"].apply(set).tolist()\n",
    "sizes = np.array([len(s) for s in sets_dates])\n",
    "keep = np.ones(len(whole_df), dtype=bool)\n",
    "idx_4 = np.where(sizes == 4)[0]\n",
    "for i in tqdm(idx_4):\n",
    "    big_set = sets_dates[i]\n",
    "    for j in range(len(whole_df)):\n",
    "        if i == j or not keep[j]:\n",
    "            continue\n",
    "        small_set = sets_dates[j]\n",
    "        if small_set.issubset(big_set):\n",
    "            keep[j] = False\n",
    "clean_df = whole_df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "clean_df.to_pickle(\"save.pkl\")\n",
    "print(len(clean_df))"
   ],
   "id": "f3f698de47996c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_pickle(\"save.pkl\")\n",
    "df = df[(df[\"latitude\"] > -13) & (df[\"latitude\"] < -12.6) & (df[\"longitude\"] > 45) & (df[\"longitude\"] < 45.7)].sort_values(\"date\")\n",
    "\n",
    "all_dates = [date for sublist in df[\"dates_orig\"] for date in sublist]\n",
    "date_counts = Counter(all_dates)\n",
    "duplicated_dates = {date for date, count in date_counts.items() if count > 1}\n",
    "to_drop = df[df[\"dates_orig\"].apply(lambda lst: any(date in duplicated_dates for date in lst))].index\n",
    "df_clean = df.drop(index=to_drop).reset_index(drop=True)\n",
    "\n",
    "df_clean.to_pickle(\"filtered_decimated.pkl\")\n",
    "print(len(df), len(df_clean))"
   ],
   "id": "964b72e93ca89ae7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_pickle(\"filtered_decimated.pkl\")\n",
    "df[\"d_seismic\"] = df.apply(lambda row: np.sqrt(np.array(row[\"d_v_seismo\"])**2 + np.array(row[\"d_h\"])**2), axis=1)\n",
    "df[\"RL_corrected\"] = df.apply(lambda row: 10 * np.array(row[\"RL\"]) + 17.201 * np.log10(np.array(row[\"d_seismic\"])), axis=1)\n",
    "df[\"SL\"] = 10*((10**(df[\"RL_corrected\"]/10)).apply(np.nanmean).apply(np.log10))\n",
    "df[\"mb\"] = -178/14.2 + df[\"SL\"] / 14.2\n",
    "df[\"depth\"] /= 1000\n",
    "df.to_csv(\"../../../../data/MAHY/loc_3D/P_association_catalog_full.csv\", index=False, columns=[\"date\",\"latitude\",\"longitude\",\"depth\",\"mb\",\"n_stations\"])\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df = df[(df[\"latitude\"] > -13) & (df[\"latitude\"] < -12.6) & (df[\"longitude\"] > 45) & (df[\"longitude\"] < 45.7)].sort_values(\"date\")\n",
    "#df['mb'] = df['mb'].map(lambda x: f\"{x:.1f}\")\n",
    "df.to_csv(\"../../../../data/MAHY/loc_3D/P_association_catalog.csv\", index=False, columns=[\"date\",\"latitude\",\"longitude\",\"depth\",\"mb\",\"SL\"], float_format=\"%.4f\")\n",
    "print(len(df))"
   ],
   "id": "a872bd4285af76b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df[\"d_seismic\"][0])\n",
    "df[\"mb\"]"
   ],
   "id": "f833e54618284b63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"SL\"]",
   "id": "2406b8a41cb907c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.sort_values(\"mb\")\n",
    "nb = np.log10(np.arange(1,1+len(df)))[::-1]\n",
    "plt.scatter(nb,df[\"mb\"])"
   ],
   "id": "4a78f8c034be3da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7bac6cd4cce0637",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
